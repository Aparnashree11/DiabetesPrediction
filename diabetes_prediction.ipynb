{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4438318",
   "metadata": {},
   "source": [
    "# Diabetes Prediction Model using Apache Beam Pipelines for Batch and Streaming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "05b79391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "from apache_beam.ml.inference.base import RunInference, ModelHandler, PredictionResult\n",
    "from apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerNumpy\n",
    "from apache_beam.io.textio import ReadFromText, WriteToText\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Iterable, Any\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010e3c0a",
   "metadata": {},
   "source": [
    "### Data Preparation for training, batch and streaming\n",
    "\n",
    "This section prepares the raw diabetes dataset and creates three distinct splits:\n",
    "- Training set: used to train the machine learning model.\n",
    "- Batch set: used to demonstrate offline/batch inference with Apache Beam.\n",
    "- Streaming set: used to simulate a real-time stream of incoming records.\n",
    "\n",
    "We use stratified sampling to preserve the original class imbalance (approx. 8.5% positive rate).\n",
    "All resulting CSVs are saved under the `data/` directory so downstream steps can reuse them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68435af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PART 1: Data Preparation - Stratified Strategy\n",
      "============================================================\n",
      "\n",
      "Original dataset: 100,000 records\n",
      "  Diabetes cases:    8,500 (8.5%)\n",
      "  No diabetes cases: 91,500 (91.5%)\n",
      "  Imbalance ratio:   1:10.8\n",
      "\n",
      "✓ Using STRATIFIED split - maintains natural distribution\n",
      "  Rationale: 5,950 positive cases is sufficient for training\n",
      "  Will use class_weight='balanced' in model training\n",
      "\n",
      "============================================================\n",
      "DATASET SPLIT SUMMARY\n",
      "============================================================\n",
      "\n",
      "Training set: 70,000 records -> data/diabetes_train.csv\n",
      "    ├─ Diabetes:    5,950 (8.5%)\n",
      "    └─ No diabetes: 64,050 (91.5%)\n",
      "\n",
      "Batch set: 20,100 records -> data/diabetes_batch.csv\n",
      "    ├─ Diabetes:    1,709 (8.5%)\n",
      "    └─ No diabetes: 18,391 (91.5%)\n",
      "\n",
      "Streaming set: 9,900 records -> data/diabetes_streaming.csv\n",
      "    ├─ Diabetes:    841 (8.5%)\n",
      "    └─ No diabetes: 9,059 (91.5%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 1: DATA PREPARATION (Run Once)\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_datasets(original_csv='diabetes_prediction_dataset.csv'):\n",
    "    \"\"\"\n",
    "    Split original dataset into training, batch, and streaming sets.\n",
    "    Uses stratified split to maintain natural 8.5% diabetes distribution.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PART 1: Data Preparation - Stratified Strategy\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    # Load original dataset\n",
    "    df = pd.read_csv(original_csv)\n",
    "    total_records = len(df)\n",
    "    diabetes_count = df['diabetes'].sum()\n",
    "    no_diabetes_count = (df['diabetes'] == 0).sum()\n",
    "    diabetes_rate = df['diabetes'].mean()\n",
    "    \n",
    "    print(f\"\\nOriginal dataset: {total_records:,} records\")\n",
    "    print(f\"  Diabetes cases:    {diabetes_count:,} ({diabetes_rate*100:.1f}%)\")\n",
    "    print(f\"  No diabetes cases: {no_diabetes_count:,} ({(1-diabetes_rate)*100:.1f}%)\")\n",
    "    print(f\"  Imbalance ratio:   1:{no_diabetes_count/diabetes_count:.1f}\")\n",
    "    \n",
    "    print(f\"\\n✓ Using STRATIFIED split - maintains natural distribution\")\n",
    "    print(f\"  Rationale: {int(diabetes_count * 0.7):,} positive cases is sufficient for training\")\n",
    "    print(f\"  Will use class_weight='balanced' in model training\")\n",
    "    \n",
    "    # Simple stratified split: 70% train, 20% batch, 10% stream\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, test_size=0.3, random_state=42, stratify=df['diabetes']\n",
    "    )\n",
    "    batch_df, stream_df = train_test_split(\n",
    "        temp_df, test_size=0.33, random_state=42, stratify=temp_df['diabetes']\n",
    "    )\n",
    "    \n",
    "    # Save splits\n",
    "    train_df.to_csv('data/diabetes_train.csv', index=False)\n",
    "    batch_df.to_csv('data/diabetes_batch.csv', index=False)\n",
    "    stream_df.to_csv('data/diabetes_streaming.csv', index=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DATASET SPLIT SUMMARY\")\n",
    "    print('='*60)\n",
    "    \n",
    "    print(f\"\\nTraining set: {len(train_df):,} records -> data/diabetes_train.csv\")\n",
    "    print(f\"    ├─ Diabetes:    {train_df['diabetes'].sum():,} ({train_df['diabetes'].mean()*100:.1f}%)\")\n",
    "    print(f\"    └─ No diabetes: {(train_df['diabetes']==0).sum():,} ({(1-train_df['diabetes'].mean())*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nBatch set: {len(batch_df):,} records -> data/diabetes_batch.csv\")\n",
    "    print(f\"    ├─ Diabetes:    {batch_df['diabetes'].sum():,} ({batch_df['diabetes'].mean()*100:.1f}%)\")\n",
    "    print(f\"    └─ No diabetes: {(batch_df['diabetes']==0).sum():,} ({(1-batch_df['diabetes'].mean())*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nStreaming set: {len(stream_df):,} records -> data/diabetes_streaming.csv\")\n",
    "    print(f\"    ├─ Diabetes:    {stream_df['diabetes'].sum():,} ({stream_df['diabetes'].mean()*100:.1f}%)\")\n",
    "    print(f\"    └─ No diabetes: {(stream_df['diabetes']==0).sum():,} ({(1-stream_df['diabetes'].mean())*100:.1f}%)\")\n",
    "\n",
    "prepare_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6900ff",
   "metadata": {},
   "source": [
    "### Model Training and storing\n",
    "\n",
    "Train ML model using a standard scikit-learn workflow (outside Apache Beam).\n",
    "This notebook trains a RandomForestClassifier and applies `class_weight='balanced'` to help with the ~8.5% positive class imbalance.\n",
    "\n",
    "Artifacts saved to the `models/` directory include:\n",
    "- `diabetes_model.pkl` — the trained Random Forest model\n",
    "- `label_encoders.pkl` — fitted LabelEncoder objects for categorical features\n",
    "- `scaler.pkl` — StandardScaler used to normalize numeric features\n",
    "- `feature_names.pkl` — ordered list of features used for inference\n",
    "\n",
    "These artifacts are loaded by the Beam pipelines to perform inference reproducibly across workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62073fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PART 2: Model Training\n",
      "============================================================\n",
      "\n",
      "Training on 70,000 records\n",
      "  Diabetes:    5,950 (8.5%)\n",
      "  No diabetes: 64,050 (91.5%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PART 2: Model Training\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load training data\n",
    "df = pd.read_csv('data/diabetes_train.csv')\n",
    "print(f\"\\nTraining on {len(df):,} records\")\n",
    "print(f\"  Diabetes:    {df['diabetes'].sum():,} ({df['diabetes'].mean()*100:.1f}%)\")\n",
    "print(f\"  No diabetes: {(df['diabetes']==0).sum():,} ({(1-df['diabetes'].mean())*100:.1f}%)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5709af37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/5] Encoding categorical variables...\n",
      "  Gender categories: ['Female', 'Male', 'Other']\n",
      "  Smoking categories: ['No Info', 'current', 'ever', 'former', 'never', 'not current']\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 1: Encode Categorical Variables\n",
    "# ========================================================================\n",
    "print(\"\\n[1/5] Encoding categorical variables...\")\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "# Encode gender\n",
    "le_gender = LabelEncoder()\n",
    "df['gender_encoded'] = le_gender.fit_transform(df['gender'])\n",
    "label_encoders['gender'] = le_gender\n",
    "print(f\"  Gender categories: {list(le_gender.classes_)}\")\n",
    "\n",
    "# Encode smoking_history\n",
    "le_smoking = LabelEncoder()\n",
    "df['smoking_encoded'] = le_smoking.fit_transform(df['smoking_history'])\n",
    "label_encoders['smoking_history'] = le_smoking\n",
    "print(f\"  Smoking categories: {list(le_smoking.classes_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c1f59a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/5] Preparing features and target...\n",
      "  Feature matrix shape: (70000, 8)\n",
      "  Target vector shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 2: Prepare Features and Target\n",
    "# ========================================================================\n",
    "print(\"\\n[2/5] Preparing features and target...\")\n",
    "\n",
    "feature_columns = [\n",
    "    'gender_encoded', \n",
    "    'age', \n",
    "    'hypertension', \n",
    "    'heart_disease', \n",
    "    'smoking_encoded', \n",
    "    'bmi', \n",
    "    'HbA1c_level', \n",
    "    'blood_glucose_level'\n",
    "]\n",
    "\n",
    "X = df[feature_columns].values\n",
    "y = df['diabetes'].values\n",
    "\n",
    "print(f\"  Feature matrix shape: {X.shape}\")\n",
    "print(f\"  Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30f66093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/5] Scaling features...\n",
      "  Features scaled using StandardScaler\n",
      "  Mean: [ 0. -0. -0.  0.  0.  0.  0.  0.]\n",
      "  Std:  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "  Training set:   56,000 samples\n",
      "  Validation set: 14,000 samples\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 3: Scale Features\n",
    "# ========================================================================\n",
    "print(\"\\n[3/5] Scaling features...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"  Features scaled using StandardScaler\")\n",
    "print(f\"  Mean: {X_scaled.mean(axis=0).round(2)}\")\n",
    "print(f\"  Std:  {X_scaled.std(axis=0).round(2)}\")\n",
    "\n",
    "# Split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n  Training set:   {len(X_train):,} samples\")\n",
    "print(f\"  Validation set: {len(X_val):,} samples\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61e002da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/5] Training Random Forest model...\n",
      "  Configuration:\n",
      "    - n_estimators: 200\n",
      "    - max_depth: 15\n",
      "    - class_weight: 'balanced' (handles 8.5% imbalance)\n",
      "    - min_samples_split: 5\n",
      "    - min_samples_leaf: 2\n",
      "\n",
      " Model training completed!\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 4: Train Random Forest Model\n",
    "# ========================================================================\n",
    "print(\"\\n[4/5] Training Random Forest model...\")\n",
    "print(\"  Configuration:\")\n",
    "print(\"    - n_estimators: 200\")\n",
    "print(\"    - max_depth: 15\")\n",
    "print(\"    - class_weight: 'balanced' (handles 8.5% imbalance)\")\n",
    "print(\"    - min_samples_split: 5\")\n",
    "print(\"    - min_samples_leaf: 2\")\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',  # Critical for imbalanced data!\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"\\n Model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eaebdf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/5] Evaluating model performance...\n",
      "\n",
      "============================================================\n",
      "VALIDATION SET PERFORMANCE\n",
      "============================================================\n",
      "\n",
      "Overall Metrics:\n",
      "  Accuracy:  0.9431\n",
      "  Precision: 0.6246\n",
      "  Recall:    0.8277\n",
      "  F1-Score:  0.7120\n",
      "  AUC-ROC:   0.9763\n",
      "\n",
      "Confusion Matrix:\n",
      "  True Negatives:  12,218\n",
      "  False Positives: 592\n",
      "  False Negatives: 205\n",
      "  True Positives:  985\n",
      "\n",
      "Additional Metrics:\n",
      "  Specificity (True Negative Rate): 0.9538\n",
      "  Negative Predictive Value:        0.9835\n",
      "\n",
      "Detailed Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " No Diabetes     0.9835    0.9538    0.9684     12810\n",
      "    Diabetes     0.6246    0.8277    0.7120      1190\n",
      "\n",
      "    accuracy                         0.9431     14000\n",
      "   macro avg     0.8041    0.8908    0.8402     14000\n",
      "weighted avg     0.9530    0.9431    0.9466     14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# STEP 5: Evaluate Model Performance\n",
    "# ========================================================================\n",
    "print(\"\\n[5/5] Evaluating model performance...\")\n",
    "\n",
    "# Predictions on validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "precision = precision_score(y_val, y_val_pred)\n",
    "recall = recall_score(y_val, y_val_pred)\n",
    "f1 = f1_score(y_val, y_val_pred)\n",
    "auc_roc = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VALIDATION SET PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOverall Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"  AUC-ROC:   {auc_roc:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives:  {tn:,}\")\n",
    "print(f\"  False Positives: {fp:,}\")\n",
    "print(f\"  False Negatives: {fn:,}\")\n",
    "print(f\"  True Positives:  {tp:,}\")\n",
    "\n",
    "# Additional insights\n",
    "specificity = tn / (tn + fp)\n",
    "npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "\n",
    "print(f\"\\nAdditional Metrics:\")\n",
    "print(f\"  Specificity (True Negative Rate): {specificity:.4f}\")\n",
    "print(f\"  Negative Predictive Value:        {npv:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_val, y_val_pred, \n",
    "    target_names=['No Diabetes', 'Diabetes'],\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac7f9f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Top Features:\n",
      "  HbA1c Level         : 0.3712 █████████████████████████████████████\n",
      "  Blood Glucose       : 0.2906 █████████████████████████████\n",
      "  Age                 : 0.1640 ████████████████\n",
      "  BMI                 : 0.0966 █████████\n",
      "  Hypertension        : 0.0311 ███\n",
      "  Smoking History     : 0.0252 ██\n",
      "  Heart Disease       : 0.0157 █\n",
      "  Gender              : 0.0055 \n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# Feature Importance Analysis\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "feature_names = [\n",
    "    'Gender', \n",
    "    'Age', \n",
    "    'Hypertension', \n",
    "    'Heart Disease', \n",
    "    'Smoking History', \n",
    "    'BMI', \n",
    "    'HbA1c Level', \n",
    "    'Blood Glucose'\n",
    "]\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop Features:\")\n",
    "for idx, row in importance_df.iterrows():\n",
    "    print(f\"  {row['Feature']:20s}: {row['Importance']:.4f} {'█' * int(row['Importance'] * 100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1cb1427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING MODEL ARTIFACTS\n",
      "============================================================\n",
      "\n",
      "Model artifacts saved:\n",
      "  - models/diabetes_model.pkl\n",
      "  - models/label_encoders.pkl\n",
      "  - models/scaler.pkl\n",
      "  - models/feature_names.pkl\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# Save Model and Preprocessing Objects\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING MODEL ARTIFACTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "joblib.dump(model, 'models/diabetes_model.pkl')\n",
    "joblib.dump(label_encoders, 'models/label_encoders.pkl')\n",
    "joblib.dump(scaler, 'models/scaler.pkl')\n",
    "\n",
    "# Save feature names for reference\n",
    "joblib.dump(feature_names, 'models/feature_names.pkl')\n",
    "\n",
    "print(\"\\nModel artifacts saved:\")\n",
    "print(\"  - models/diabetes_model.pkl\")\n",
    "print(\"  - models/label_encoders.pkl\")\n",
    "print(\"  - models/scaler.pkl\")\n",
    "print(\"  - models/feature_names.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a0ccffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL SUMMARY\n",
      "============================================================\n",
      "\n",
      "Model Type: Random Forest Classifier\n",
      "Training Samples: 56,000\n",
      "Validation Samples: 14,000\n",
      "Number of Features: 8\n",
      "Number of Trees: 200\n",
      "Best Validation AUC-ROC: 0.9763\n",
      "\n",
      "Model is ready for Apache Beam inference pipeline!\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# Model Summary\n",
    "# ========================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nModel Type: Random Forest Classifier\")\n",
    "print(f\"Training Samples: {len(X_train):,}\")\n",
    "print(f\"Validation Samples: {len(X_val):,}\")\n",
    "print(f\"Number of Features: {len(feature_columns)}\")\n",
    "print(f\"Number of Trees: {model.n_estimators}\")\n",
    "print(f\"Best Validation AUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"\\nModel is ready for Apache Beam inference pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae602f",
   "metadata": {},
   "source": [
    "### Custom Transforms for Prediction\n",
    "\n",
    "This section defines reusable Apache Beam transforms (DoFn classes) used by both the batch and streaming pipelines.\n",
    "Key responsibilities implemented here include:\n",
    "- Parsing CSV lines into structured dictionaries\n",
    "- Preprocessing records (encoding categorical values and scaling numeric features),\n",
    "- Formatting examples for the RunInference API,\n",
    "- Filtering and flagging high-risk patients, and\n",
    "- Calculating aggregated metrics or windowed summaries.\n",
    "\n",
    "Design note: heavy resources (model, encoders, scaler) are loaded once per worker in `setup()` to minimize overhead during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0cb3df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRANSFORM 1: Parse CSV Data\n",
    "# ============================================================================\n",
    "\n",
    "class ParseCSV(beam.DoFn):\n",
    "    \"\"\"Parse CSV line into dictionary.\"\"\"\n",
    "    \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def process(self, line):\n",
    "        try:\n",
    "            values = line.strip().split(',')\n",
    "            if len(values) != len(self.columns):\n",
    "                return\n",
    "            \n",
    "            record = dict(zip(self.columns, values))\n",
    "            \n",
    "            # Convert numeric fields\n",
    "            record['age'] = float(record['age'])\n",
    "            record['hypertension'] = int(record['hypertension'])\n",
    "            record['heart_disease'] = int(record['heart_disease'])\n",
    "            record['bmi'] = float(record['bmi'])\n",
    "            record['HbA1c_level'] = float(record['HbA1c_level'])\n",
    "            record['blood_glucose_level'] = float(record['blood_glucose_level'])\n",
    "            \n",
    "            if 'diabetes' in record and record['diabetes'] != '':\n",
    "                record['diabetes'] = int(record['diabetes'])\n",
    "            \n",
    "            yield record\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing line: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d028e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRANSFORM 2: Preprocess for Inference\n",
    "# ============================================================================\n",
    "\n",
    "class PreprocessForInference(beam.DoFn):\n",
    "    \"\"\"Preprocess data for model inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, encoders_path, scaler_path):\n",
    "        self.encoders_path = encoders_path\n",
    "        self.scaler_path = scaler_path\n",
    "        self.encoders = None\n",
    "        self.scaler = None\n",
    "    \n",
    "    def setup(self):\n",
    "        \"\"\"Load encoders and scaler once per worker.\"\"\"\n",
    "        self.encoders = joblib.load(self.encoders_path)\n",
    "        self.scaler = joblib.load(self.scaler_path)\n",
    "    \n",
    "    def process(self, record):\n",
    "        try:\n",
    "            # Encode categorical features\n",
    "            gender_encoded = self.encoders['gender'].transform([record['gender']])[0]\n",
    "            smoking_encoded = self.encoders['smoking_history'].transform([record['smoking_history']])[0]\n",
    "            \n",
    "            # Create feature vector\n",
    "            features = np.array([\n",
    "                gender_encoded,\n",
    "                record['age'],\n",
    "                record['hypertension'],\n",
    "                record['heart_disease'],\n",
    "                smoking_encoded,\n",
    "                record['bmi'],\n",
    "                record['HbA1c_level'],\n",
    "                record['blood_glucose_level']\n",
    "            ]).reshape(1, -1)\n",
    "            \n",
    "            # Scale features\n",
    "            features_scaled = self.scaler.transform(features)\n",
    "            \n",
    "            yield (record, features_scaled[0])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Preprocessing error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c208d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRANSFORM 3: Format for Model Inference\n",
    "# ============================================================================\n",
    "\n",
    "class FormatForInference(beam.DoFn):\n",
    "    \"\"\"Format features for RunInference API.\"\"\"\n",
    "    \n",
    "    def process(self, element):\n",
    "        record, features = element\n",
    "        yield np.array(features).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRANSFORM 4: Filter High-Risk Patients\n",
    "# ============================================================================\n",
    "\n",
    "class FilterHighRisk(beam.DoFn):\n",
    "    \"\"\"Filter high-risk patients (predicted diabetes = 1).\"\"\"\n",
    "    \n",
    "    def process(self, record):\n",
    "        if record['predicted_diabetes'] == 1:\n",
    "            yield record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ea468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRANSFORM 5: Calculate Metrics\n",
    "# ============================================================================\n",
    "\n",
    "class CalculateMetrics(beam.DoFn):\n",
    "    \"\"\"Calculate prediction metrics and statistics.\"\"\"\n",
    "    \n",
    "    def process(self, records):\n",
    "        \"\"\"Calculate metrics from a list of prediction records.\"\"\"\n",
    "        records_list = list(records)\n",
    "        \n",
    "        if not records_list:\n",
    "            return\n",
    "        \n",
    "        total = len(records_list)\n",
    "        predicted_positive = sum(1 for r in records_list if r['predicted_diabetes'] == 1)\n",
    "        predicted_negative = total - predicted_positive\n",
    "        \n",
    "        # Check if actual labels are available\n",
    "        has_labels = records_list[0]['actual_diabetes'] is not None\n",
    "        \n",
    "        if has_labels:\n",
    "            # Calculate confusion matrix components\n",
    "            tp = sum(1 for r in records_list \n",
    "                    if r['predicted_diabetes'] == 1 and r['actual_diabetes'] == 1)\n",
    "            tn = sum(1 for r in records_list \n",
    "                    if r['predicted_diabetes'] == 0 and r['actual_diabetes'] == 0)\n",
    "            fp = sum(1 for r in records_list \n",
    "                    if r['predicted_diabetes'] == 1 and r['actual_diabetes'] == 0)\n",
    "            fn = sum(1 for r in records_list \n",
    "                    if r['predicted_diabetes'] == 0 and r['actual_diabetes'] == 1)\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            accuracy = (tp + tn) / total if total > 0 else 0\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            actual_positive = sum(1 for r in records_list if r['actual_diabetes'] == 1)\n",
    "            \n",
    "            metrics = {\n",
    "                'total_predictions': total,\n",
    "                'predicted_positive': predicted_positive,\n",
    "                'predicted_negative': predicted_negative,\n",
    "                'actual_positive': actual_positive,\n",
    "                'actual_negative': total - actual_positive,\n",
    "                'true_positives': tp,\n",
    "                'true_negatives': tn,\n",
    "                'false_positives': fp,\n",
    "                'false_negatives': fn,\n",
    "                'accuracy': round(accuracy, 4),\n",
    "                'precision': round(precision, 4),\n",
    "                'recall': round(recall, 4),\n",
    "                'f1_score': round(f1, 4),\n",
    "                'high_risk_percentage': round((predicted_positive / total) * 100, 2),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        else:\n",
    "            # Metrics without ground truth labels\n",
    "            metrics = {\n",
    "                'total_predictions': total,\n",
    "                'predicted_positive': predicted_positive,\n",
    "                'predicted_negative': predicted_negative,\n",
    "                'high_risk_percentage': round((predicted_positive / total) * 100, 2),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        yield metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e8ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRANSFORM 6: Filter High-Risk Patients\n",
    "# ============================================================================\n",
    "\n",
    "class FilterHighRisk(beam.DoFn):\n",
    "    \"\"\"Filter and flag high-risk patients for follow-up.\"\"\"\n",
    "    \n",
    "    def process(self, record):\n",
    "        \"\"\"Filter high-risk patients and assign priority.\"\"\"\n",
    "        if record['predicted_diabetes'] == 1:\n",
    "            # Determine priority based on multiple factors\n",
    "            priority_score = 0\n",
    "            \n",
    "            # High HbA1c\n",
    "            if record['HbA1c_level'] >= 6.5:\n",
    "                priority_score += 3\n",
    "            \n",
    "            # High blood glucose\n",
    "            if record['blood_glucose_level'] >= 200:\n",
    "                priority_score += 3\n",
    "            \n",
    "            # Obesity\n",
    "            if record['bmi'] >= 30:\n",
    "                priority_score += 2\n",
    "            \n",
    "            # Comorbidities\n",
    "            if record['hypertension']:\n",
    "                priority_score += 1\n",
    "            if record['heart_disease']:\n",
    "                priority_score += 1\n",
    "            \n",
    "            # Age factor\n",
    "            if record['age'] >= 60:\n",
    "                priority_score += 1\n",
    "            \n",
    "            # Assign priority level\n",
    "            if priority_score >= 7:\n",
    "                priority = 'CRITICAL'\n",
    "            elif priority_score >= 5:\n",
    "                priority = 'HIGH'\n",
    "            else:\n",
    "                priority = 'MEDIUM'\n",
    "            \n",
    "            # Add priority information\n",
    "            record['priority'] = priority\n",
    "            record['priority_score'] = priority_score\n",
    "            record['recommended_action'] = 'Immediate medical consultation required'\n",
    "            \n",
    "            yield record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a92ef",
   "metadata": {},
   "source": [
    "### Apache Beam Batch Prediction Pipeline\n",
    "\n",
    "This pipeline shows how to perform offline/batch inference with Apache Beam using the trained model artifacts.\n",
    "Steps include reading and parsing the CSV batch file, preprocessing records in parallel, running model inference via the RunInference API, and writing both full prediction results and a CSV of high-risk patients.\n",
    "\n",
    "Outputs are written under `output/batch_predictions/` and include a JSONL file with all predictions and a CSV file listing high-risk patients for follow-up. The pipeline runs locally with the DirectRunner by default but can be adapted to other Beam runners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CUSTOM MODEL HANDLER\n",
    "# ============================================================================\n",
    "\n",
    "class DiabetesModelHandler(ModelHandler):\n",
    "    \"\"\"Custom model handler for diabetes prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self._model = None\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the model once per worker.\"\"\"\n",
    "        self._model = joblib.load(self.model_path)\n",
    "        return self._model\n",
    "    \n",
    "    def run_inference(self, batch: Iterable[np.ndarray], model: Any, inference_args=None) -> Iterable[PredictionResult]:\n",
    "        \"\"\"Run inference on a batch.\"\"\"\n",
    "        predictions = []\n",
    "        for example in batch:\n",
    "            if len(example.shape) == 1:\n",
    "                example = example.reshape(1, -1)\n",
    "            pred = model.predict(example)\n",
    "            predictions.append(PredictionResult(example=example, inference=pred))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33bb2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def combine_results(preprocessed_item, predictions):\n",
    "    \"\"\"Combine preprocessed record with prediction.\"\"\"\n",
    "    record, features = preprocessed_item\n",
    "    \n",
    "    # Find corresponding prediction (same index)\n",
    "    idx = 0  # Simplified: assumes order is preserved\n",
    "    if idx < len(predictions):\n",
    "        pred = predictions[idx]\n",
    "        predicted_class = int(pred.inference[0])\n",
    "    else:\n",
    "        predicted_class = 0\n",
    "    \n",
    "    return {\n",
    "        'patient_id': f\"P{hash(str(record)) % 1000000:06d}\",\n",
    "        'gender': record['gender'],\n",
    "        'age': int(record['age']),\n",
    "        'bmi': round(record['bmi'], 2),\n",
    "        'HbA1c_level': round(record['HbA1c_level'], 2),\n",
    "        'blood_glucose_level': int(record['blood_glucose_level']),\n",
    "        'smoking_history': record['smoking_history'],\n",
    "        'hypertension': bool(record['hypertension']),\n",
    "        'heart_disease': bool(record['heart_disease']),\n",
    "        'predicted_diabetes': predicted_class,\n",
    "        'risk_category': 'HIGH' if predicted_class == 1 else 'LOW',\n",
    "        'actual_diabetes': record.get('diabetes', None),\n",
    "    }\n",
    "\n",
    "\n",
    "def format_csv(record):\n",
    "    \"\"\"Format record as CSV line.\"\"\"\n",
    "    return f\"{record['patient_id']},\" \\\n",
    "           f\"{record['gender']},\" \\\n",
    "           f\"{record['age']},\" \\\n",
    "           f\"{record['bmi']},\" \\\n",
    "           f\"{record['HbA1c_level']},\" \\\n",
    "           f\"{record['blood_glucose_level']},\" \\\n",
    "           f\"{record['predicted_diabetes']},\" \\\n",
    "           f\"{record.get('actual_diabetes', 'N/A')}\"\n",
    "\n",
    "\n",
    "def count_lines(filepath):\n",
    "    \"\"\"Count lines in file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            return sum(1 for _ in f)\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f7af11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "APACHE BEAM BATCH PREDICTION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Input:  data/diabetes_batch.csv\n",
      "Output: output/batch_predictions/\n",
      "\n",
      "Executing pipeline...\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Resource module is not available for current platform, memory usage cannot be fetched.\n",
      "WARNING:root:Resource module is not available for current platform, memory usage cannot be fetched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PIPELINE COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "Output Files:\n",
      "  output/batch_predictions/predictions.jsonl\n",
      "    - All 20100 predictions\n",
      "  \n",
      "  output/batch_predictions/high_risk_patients.csv\n",
      "    - High-risk patients (predicted diabetes = 1)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BATCH PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_batch_pipeline(\n",
    "    input_file='data/diabetes_batch.csv',\n",
    "    output_dir='output/batch_predictions',\n",
    "    model_path='models/diabetes_model.pkl',\n",
    "    encoders_path='models/label_encoders.pkl',\n",
    "    scaler_path='models/scaler.pkl'\n",
    "):\n",
    "    \"\"\"\n",
    "    Batch prediction pipeline.\n",
    "    \n",
    "    Steps:\n",
    "    1. Read and parse CSV\n",
    "    2. Preprocess (encode + scale)\n",
    "    3. Run ML inference\n",
    "    4. Store all predictions\n",
    "    5. Filter high-risk patients\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"APACHE BEAM BATCH PREDICTION PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nInput:  {input_file}\")\n",
    "    print(f\"Output: {output_dir}/\")\n",
    "    print(\"\\nExecuting pipeline...\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # CSV columns\n",
    "    columns = [\n",
    "        'gender', 'age', 'hypertension', 'heart_disease', \n",
    "        'smoking_history', 'bmi', 'HbA1c_level', \n",
    "        'blood_glucose_level', 'diabetes'\n",
    "    ]\n",
    "    \n",
    "    # Model handler\n",
    "    model_handler = DiabetesModelHandler(model_path)\n",
    "    \n",
    "    # Pipeline options\n",
    "    options = PipelineOptions(['--runner=DirectRunner'])\n",
    "    \n",
    "    with beam.Pipeline(options=options) as pipeline:\n",
    "        \n",
    "        # Step 1: Read and parse\n",
    "        parsed = (\n",
    "            pipeline\n",
    "            | 'Read CSV' >> ReadFromText(input_file, skip_header_lines=1)\n",
    "            | 'Parse' >> beam.ParDo(ParseCSV(columns))\n",
    "        )\n",
    "        \n",
    "        # Step 2: Preprocess\n",
    "        preprocessed = (\n",
    "            parsed\n",
    "            | 'Preprocess' >> beam.ParDo(\n",
    "                PreprocessForInference(encoders_path, scaler_path)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Step 3: Run inference\n",
    "        predictions = (\n",
    "            preprocessed\n",
    "            | 'Format Features' >> beam.ParDo(FormatForInference())\n",
    "            | 'Predict' >> RunInference(model_handler)\n",
    "        )\n",
    "        \n",
    "        # Step 4: Combine and create results\n",
    "        results = (\n",
    "            preprocessed\n",
    "            | 'Combine with Predictions' >> beam.Map(\n",
    "                combine_results,\n",
    "                predictions=beam.pvalue.AsList(predictions)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Step 5: Store all predictions\n",
    "        (\n",
    "            results\n",
    "            | 'Format JSON' >> beam.Map(lambda r: json.dumps(r))\n",
    "            | 'Write All Predictions' >> WriteToText(\n",
    "                f'{output_dir}/predictions',\n",
    "                file_name_suffix='.jsonl',\n",
    "                shard_name_template=''\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Step 6: Filter and store high-risk patients\n",
    "        (\n",
    "            results\n",
    "            | 'Filter High Risk' >> beam.ParDo(FilterHighRisk())\n",
    "            | 'Format CSV' >> beam.Map(format_csv)\n",
    "            | 'Write High Risk' >> WriteToText(\n",
    "                f'{output_dir}/high_risk_patients',\n",
    "                file_name_suffix='.csv',\n",
    "                header='patient_id,gender,age,bmi,HbA1c_level,blood_glucose_level,predicted_diabetes,actual_diabetes',\n",
    "                shard_name_template=''\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PIPELINE COMPLETED!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nOutput Files:\")\n",
    "    print(f\"  {output_dir}/predictions.jsonl\")\n",
    "    print(f\"    - All {count_lines(f'{output_dir}/predictions.jsonl')} predictions\")\n",
    "    print(f\"  \")\n",
    "    print(f\"  {output_dir}/high_risk_patients.csv\")\n",
    "    print(f\"    - High-risk patients (predicted diabetes = 1)\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "run_batch_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab8203b",
   "metadata": {},
   "source": [
    "### Streaming Pipeline\n",
    "\n",
    "This section simulates a near-real-time inference pipeline using Apache Beam primitives suited for streaming workloads.\n",
    "The pipeline reads records (simulated from a CSV file), attaches timestamps, windows the stream, preprocesses each record, runs inference, and emits continuous outputs and alerts for high-risk patients.\n",
    "\n",
    "Streaming outputs are written to `output/streaming_predictions/` and include a rolling JSONL of predictions, a JSONL of real-time high-risk alerts, and a periodic summary file with windowed metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5075e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STREAMING-SPECIFIC TRANSFORMS\n",
    "# ============================================================================\n",
    "\n",
    "class AddTimestamp(beam.DoFn):\n",
    "    \"\"\"Add timestamp to each record for windowing.\"\"\"\n",
    "    \n",
    "    def process(self, element):\n",
    "        # Add current timestamp\n",
    "        yield beam.window.TimestampedValue(element, time.time())\n",
    "\n",
    "\n",
    "class FormatStreamingOutput(beam.DoFn):\n",
    "    \"\"\"Format output with streaming metadata.\"\"\"\n",
    "    \n",
    "    def process(self, element):\n",
    "        record, features = element\n",
    "        \n",
    "        result = {\n",
    "            'stream_id': f\"S{int(time.time() * 1000000) % 1000000:06d}\",\n",
    "            'patient_id': f\"P{hash(str(record)) % 1000000:06d}\",\n",
    "            'gender': record['gender'],\n",
    "            'age': int(record['age']),\n",
    "            'bmi': round(record['bmi'], 2),\n",
    "            'HbA1c_level': round(record['HbA1c_level'], 2),\n",
    "            'blood_glucose_level': int(record['blood_glucose_level']),\n",
    "            'smoking_history': record['smoking_history'],\n",
    "            'processed_time': datetime.now().isoformat(),\n",
    "            'window': 'streaming'\n",
    "        }\n",
    "        \n",
    "        yield result\n",
    "\n",
    "\n",
    "class AddPredictionToRecord(beam.DoFn):\n",
    "    \"\"\"Add prediction to streaming record.\"\"\"\n",
    "    \n",
    "    def process(self, element, predictions):\n",
    "        if not predictions:\n",
    "            return\n",
    "        \n",
    "        # Get first prediction\n",
    "        pred = predictions[0]\n",
    "        predicted_class = int(pred.inference[0])\n",
    "        \n",
    "        element['predicted_diabetes'] = predicted_class\n",
    "        element['risk_category'] = 'HIGH' if predicted_class == 1 else 'LOW'\n",
    "        element['prediction_time'] = datetime.now().isoformat()\n",
    "        \n",
    "        yield element\n",
    "\n",
    "\n",
    "class CountPredictions(beam.DoFn):\n",
    "    \"\"\"Count predictions in each window.\"\"\"\n",
    "    \n",
    "    def process(self, elements):\n",
    "        elements_list = list(elements)\n",
    "        total = len(elements_list)\n",
    "        high_risk = sum(1 for e in elements_list if e.get('predicted_diabetes') == 1)\n",
    "        \n",
    "        summary = {\n",
    "            'window_summary': True,\n",
    "            'total_processed': total,\n",
    "            'high_risk_count': high_risk,\n",
    "            'low_risk_count': total - high_risk,\n",
    "            'high_risk_percentage': round((high_risk / total * 100), 2) if total > 0 else 0,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        yield summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a56a0a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def combine_streaming_results(preprocessed_item, predictions):\n",
    "    \"\"\"Combine preprocessed record with prediction.\"\"\"\n",
    "    record, features = preprocessed_item\n",
    "    \n",
    "    # Get prediction\n",
    "    idx = 0\n",
    "    if idx < len(predictions):\n",
    "        pred = predictions[idx]\n",
    "        predicted_class = int(pred.inference[0])\n",
    "    else:\n",
    "        predicted_class = 0\n",
    "    \n",
    "    return {\n",
    "        'stream_id': f\"S{int(time.time() * 1000000) % 1000000:06d}\",\n",
    "        'patient_id': f\"P{hash(str(record)) % 1000000:06d}\",\n",
    "        'gender': record['gender'],\n",
    "        'age': int(record['age']),\n",
    "        'bmi': round(record['bmi'], 2),\n",
    "        'HbA1c_level': round(record['HbA1c_level'], 2),\n",
    "        'blood_glucose_level': int(record['blood_glucose_level']),\n",
    "        'smoking_history': record['smoking_history'],\n",
    "        'hypertension': bool(record['hypertension']),\n",
    "        'heart_disease': bool(record['heart_disease']),\n",
    "        'predicted_diabetes': predicted_class,\n",
    "        'risk_category': 'HIGH' if predicted_class == 1 else 'LOW',\n",
    "        'actual_diabetes': record.get('diabetes', None),\n",
    "        'processed_time': datetime.now().isoformat()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a6343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "APACHE BEAM STREAMING PREDICTION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Input:  data/diabetes_streaming.csv\n",
      "Output: output/streaming_predictions/\n",
      "Window: 10 seconds\n",
      "\n",
      "Simulating real-time streaming...\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Resource module is not available for current platform, memory usage cannot be fetched.\n",
      "WARNING:root:Resource module is not available for current platform, memory usage cannot be fetched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STREAMING PIPELINE COMPLETED!\n",
      "======================================================================\n",
      "\n",
      "Output Files:\n",
      "  output/streaming_predictions/stream_predictions.jsonl\n",
      "    - All streaming predictions\n",
      "  \n",
      "  output/streaming_predictions/high_risk_alerts.jsonl\n",
      "    - Real-time alerts for high-risk patients\n",
      "  \n",
      "  output/streaming_predictions/stream_summary.json\n",
      "    - Summary statistics\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STREAMING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_streaming_pipeline(\n",
    "    input_file='data/diabetes_streaming.csv',\n",
    "    output_dir='output/streaming_predictions',\n",
    "    model_path='models/diabetes_model.pkl',\n",
    "    encoders_path='models/label_encoders.pkl',\n",
    "    scaler_path='models/scaler.pkl',\n",
    "    window_size_seconds=10\n",
    "):\n",
    "    \"\"\"Run streaming prediction pipeline.\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"APACHE BEAM STREAMING PREDICTION PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nInput:  {input_file}\")\n",
    "    print(f\"Output: {output_dir}/\")\n",
    "    print(f\"Window: {window_size_seconds} seconds\")\n",
    "    print(\"\\nSimulating real-time streaming...\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # CSV columns\n",
    "    columns = [\n",
    "        'gender', 'age', 'hypertension', 'heart_disease', \n",
    "        'smoking_history', 'bmi', 'HbA1c_level', \n",
    "        'blood_glucose_level', 'diabetes'\n",
    "    ]\n",
    "    \n",
    "    # Model handler\n",
    "    model_handler = DiabetesModelHandler(model_path)\n",
    "    \n",
    "    # Pipeline options for streaming\n",
    "    options = PipelineOptions([\n",
    "        '--runner=DirectRunner'\n",
    "    ])\n",
    "    \n",
    "    with beam.Pipeline(options=options) as pipeline:\n",
    "        \n",
    "        # Step 1: Read and parse (simulates incoming stream)\n",
    "        parsed = (\n",
    "            pipeline\n",
    "            | 'Read Stream' >> ReadFromText(input_file, skip_header_lines=1)\n",
    "            | 'Parse' >> beam.ParDo(ParseCSV(columns))\n",
    "        )\n",
    "        \n",
    "        # Step 2: Preprocess\n",
    "        preprocessed = (\n",
    "            parsed\n",
    "            | 'Preprocess' >> beam.ParDo(\n",
    "                PreprocessForInference(encoders_path, scaler_path)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Step 3: Run inference\n",
    "        predictions = (\n",
    "            preprocessed\n",
    "            | 'Format Features' >> beam.ParDo(FormatForInference())\n",
    "            | 'Predict' >> RunInference(model_handler)\n",
    "        )\n",
    "        \n",
    "        # Step 4: Combine results\n",
    "        results = (\n",
    "            preprocessed\n",
    "            | 'Combine Results' >> beam.Map(\n",
    "                combine_streaming_results,\n",
    "                predictions=beam.pvalue.AsList(predictions)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Step 5: Write all streaming predictions\n",
    "        (\n",
    "            results\n",
    "            | 'Format JSON' >> beam.Map(lambda r: json.dumps(r))\n",
    "            | 'Write Stream Predictions' >> WriteToText(\n",
    "                f'{output_dir}/stream_predictions',\n",
    "                file_name_suffix='.jsonl',\n",
    "                shard_name_template=''\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Step 6: Filter and write high-risk alerts\n",
    "        (\n",
    "            results\n",
    "            | 'Filter High Risk Stream' >> beam.ParDo(FilterHighRisk())\n",
    "            | 'Format Alert' >> beam.Map(\n",
    "                lambda r: json.dumps({\n",
    "                    'ALERT': 'HIGH RISK PATIENT',\n",
    "                    'patient_id': r['patient_id'],\n",
    "                    'age': r['age'],\n",
    "                    'HbA1c_level': r['HbA1c_level'],\n",
    "                    'blood_glucose_level': r['blood_glucose_level'],\n",
    "                    'alert_time': datetime.now().isoformat()\n",
    "                })\n",
    "            )\n",
    "            | 'Write Alerts' >> WriteToText(\n",
    "                f'{output_dir}/high_risk_alerts',\n",
    "                file_name_suffix='.jsonl',\n",
    "                shard_name_template=''\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Step 7: Create summary statistics\n",
    "        (\n",
    "            results\n",
    "            | 'Group for Summary' >> beam.combiners.ToList()\n",
    "            | 'Calculate Summary' >> beam.ParDo(CountPredictions())\n",
    "            | 'Format Summary' >> beam.Map(lambda s: json.dumps(s, indent=2))\n",
    "            | 'Write Summary' >> WriteToText(\n",
    "                f'{output_dir}/stream_summary',\n",
    "                file_name_suffix='.json',\n",
    "                shard_name_template=''\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STREAMING PIPELINE COMPLETED!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nOutput Files:\")\n",
    "    print(f\"  {output_dir}/stream_predictions.jsonl\")\n",
    "    print(f\"    - All streaming predictions\")\n",
    "    print(f\"  \")\n",
    "    print(f\"  {output_dir}/high_risk_alerts.jsonl\")\n",
    "    print(f\"    - Real-time alerts for high-risk patients\")\n",
    "    print(f\"  \")\n",
    "    print(f\"  {output_dir}/stream_summary.json\")\n",
    "    print(f\"    - Summary statistics\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "\n",
    "run_streaming_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
